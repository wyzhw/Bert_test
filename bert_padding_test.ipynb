{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWLNrzM0Noqw"
      },
      "source": [
        "# Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OJR_RTQNoqy"
      },
      "source": [
        "Text classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis, which assigns a label like üôÇ positive, üôÅ negative, or üòê neutral to a sequence of text.\n",
        "\n",
        "This guide will show you how to:\n",
        "\n",
        "1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [IMDb](https://huggingface.co/datasets/imdb) dataset to determine whether a movie review is positive or negative.\n",
        "2. Use your finetuned model for inference.\n",
        "\n",
        "<Tip>\n",
        "The task illustrated in this tutorial is supported by the following model architectures:\n",
        "\n",
        "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
        "\n",
        "[ALBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/albert), [BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bert), [BigBird](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/big_bird), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [BioGpt](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/biogpt), [BLOOM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bloom), [CamemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/camembert), [CANINE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/canine), [ConvBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/convbert), [CTRL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ctrl), [Data2VecText](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/data2vec-text), [DeBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta), [DeBERTa-v2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta-v2), [DistilBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/distilbert), [ELECTRA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/electra), [ERNIE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie), [ErnieM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie_m), [ESM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/esm), [FlauBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/flaubert), [FNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fnet), [Funnel Transformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/funnel), [GPT-Sw3](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt-sw3), [OpenAI GPT-2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt2), [GPTBigCode](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_bigcode), [GPT Neo](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neo), [GPT NeoX](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neox), [GPT-J](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptj), [I-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ibert), [LayoutLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlm), [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv2), [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv3), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LiLT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/lilt), [LLaMA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/llama), [Longformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longformer), [LUKE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/luke), [MarkupLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/markuplm), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MEGA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mega), [Megatron-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/megatron-bert), [MobileBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mobilebert), [MPNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mpnet), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [Nezha](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nezha), [Nystr√∂mformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nystromformer), [OpenLlama](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/open-llama), [OpenAI GPT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/openai-gpt), [OPT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/opt), [Perceiver](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/perceiver), [PLBart](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/plbart), [QDQBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/qdqbert), [Reformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/reformer), [RemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/rembert), [RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta), [RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta-prelayernorm), [RoCBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roc_bert), [RoFormer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roformer), [SqueezeBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/squeezebert), [TAPAS](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/tapas), [Transformer-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/transfo-xl), [XLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm), [XLM-RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta), [XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta-xl), [XLNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlnet), [X-MOD](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xmod), [YOSO](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/yoso)\n",
        "\n",
        "\n",
        "<!--End of the generated tip-->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed:\n",
        "\n",
        "```bash\n",
        "pip install transformers datasets evaluate\n",
        "```\n",
        "\n",
        "We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m0moU-wINoqz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d949b34a47a44fdadf42d7ec8511b20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "#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"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUpFDnxpNoqz"
      },
      "source": [
        "## Load IMDb dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7QWQHleNoq0"
      },
      "source": [
        "Start by loading the IMDb dataset from the ü§ó Datasets library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yi6Jrqw4Noq0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q6UaVksNoq0"
      },
      "source": [
        "Then take a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3P15-m40Noq0",
        "outputId": "a654a224-3712-4640-8d88-8e730a9d80f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1ZoRuiENoq1"
      },
      "source": [
        "There are two fields in this dataset:\n",
        "\n",
        "- `text`: the movie review text.\n",
        "- `label`: a value that is either `0` for a negative review or `1` for a positive review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGSkJSQ5Noq1"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcVgwUOdNoq1"
      },
      "source": [
        "The next step is to load a DistilBERT tokenizer to preprocess the `text` field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9YTQxBulNoq1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxsQUau7Noq1"
      },
      "source": [
        "Create a preprocessing function to tokenize `text` and truncate sequences to be no longer than DistilBERT's maximum input length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IOLplL98Noq1"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add different paddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "imdb_padding_0 3\n",
            "imdb_padding_10 3\n",
            "imdb_padding_20 3\n",
            "imdb_padding_30 3\n",
            "imdb_padding_40 3\n",
            "imdb_padding_50 3\n",
            "imdb_padding_60 3\n",
            "imdb_padding_70 3\n",
            "imdb_padding_80 3\n",
            "imdb_padding_90 3\n",
            "imdb_padding_100 3\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")\n",
        "\n",
        "def add_text(example, n):\n",
        "  # example is a dictionary with keys 'text' and 'label'\n",
        "  # we can modify the 'text' value and return the modified example\n",
        "  example['text'] = \"[PAD]\" * n + example['text']\n",
        "  return example\n",
        "\n",
        "for n in range(0, 101, 10):\n",
        "    # use format method to create a variable name with n\n",
        "    var_name = \"imdb_padding_{}\".format(n)\n",
        "    # use exec function to execute the assignment statement\n",
        "    exec(\"{} = imdb.map(lambda x: add_text(x, n))\".format(var_name))\n",
        "    print(var_name, len(eval(var_name)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "imdb_padding_150 3\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")\n",
        "\n",
        "def add_text(example, n):\n",
        "  # example is a dictionary with keys 'text' and 'label'\n",
        "  # we can modify the 'text' value and return the modified example\n",
        "  example['text'] = \"[PAD]\" * n + example['text']\n",
        "  return example\n",
        "\n",
        "for n in range(150, 151, 1):\n",
        "    # use format method to create a variable name with n\n",
        "    var_name = \"imdb_padding_{}\".format(n)\n",
        "    # use exec function to execute the assignment statement\n",
        "    exec(\"{} = imdb.map(lambda x: add_text(x, n))\".format(var_name))\n",
        "    print(var_name, len(eval(var_name)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'imdb_padding150' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\wyzhw\\Documents\\GitHub\\Bert_test\\bert_padding_test.ipynb ÂçïÂÖÉÊ†º 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m imdb_padding150[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'imdb_padding150' is not defined"
          ]
        }
      ],
      "source": [
        "imdb_padding150[\"test\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': '[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb_padding_10[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCmFugV6Noq1"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ü§ó Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mGowaDQUNoq1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e763ba276e74e7db7fe709f4b1d898d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "n_range = range(0, 101, 10)\n",
        "\n",
        "for n in n_range:\n",
        "    filename = f\"imdb_padding_{n}\"\n",
        "    file = eval(filename)\n",
        "    tokenized_filename = f\"tokenized_imdb_{n}\"\n",
        "    exec(f\"{tokenized_filename} = file.map(preprocess_function, batched=True)\")\n",
        "    tokenized_file = eval(tokenized_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': '[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
              " 'label': 0,\n",
              " 'input_ids': [101,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1045,\n",
              "  2293,\n",
              "  16596,\n",
              "  1011,\n",
              "  10882,\n",
              "  1998,\n",
              "  2572,\n",
              "  5627,\n",
              "  2000,\n",
              "  2404,\n",
              "  2039,\n",
              "  2007,\n",
              "  1037,\n",
              "  2843,\n",
              "  1012,\n",
              "  16596,\n",
              "  1011,\n",
              "  10882,\n",
              "  5691,\n",
              "  1013,\n",
              "  2694,\n",
              "  2024,\n",
              "  2788,\n",
              "  2104,\n",
              "  11263,\n",
              "  25848,\n",
              "  1010,\n",
              "  2104,\n",
              "  1011,\n",
              "  12315,\n",
              "  1998,\n",
              "  28947,\n",
              "  1012,\n",
              "  1045,\n",
              "  2699,\n",
              "  2000,\n",
              "  2066,\n",
              "  2023,\n",
              "  1010,\n",
              "  1045,\n",
              "  2428,\n",
              "  2106,\n",
              "  1010,\n",
              "  2021,\n",
              "  2009,\n",
              "  2003,\n",
              "  2000,\n",
              "  2204,\n",
              "  2694,\n",
              "  16596,\n",
              "  1011,\n",
              "  10882,\n",
              "  2004,\n",
              "  17690,\n",
              "  1019,\n",
              "  2003,\n",
              "  2000,\n",
              "  2732,\n",
              "  10313,\n",
              "  1006,\n",
              "  1996,\n",
              "  2434,\n",
              "  1007,\n",
              "  1012,\n",
              "  10021,\n",
              "  4013,\n",
              "  3367,\n",
              "  20086,\n",
              "  2015,\n",
              "  1010,\n",
              "  10036,\n",
              "  19747,\n",
              "  4520,\n",
              "  1010,\n",
              "  25931,\n",
              "  3064,\n",
              "  22580,\n",
              "  1010,\n",
              "  1039,\n",
              "  2290,\n",
              "  2008,\n",
              "  2987,\n",
              "  1005,\n",
              "  1056,\n",
              "  2674,\n",
              "  1996,\n",
              "  4281,\n",
              "  1010,\n",
              "  1998,\n",
              "  16267,\n",
              "  2028,\n",
              "  1011,\n",
              "  8789,\n",
              "  3494,\n",
              "  3685,\n",
              "  2022,\n",
              "  9462,\n",
              "  2007,\n",
              "  1037,\n",
              "  1005,\n",
              "  16596,\n",
              "  1011,\n",
              "  10882,\n",
              "  1005,\n",
              "  4292,\n",
              "  1012,\n",
              "  1006,\n",
              "  1045,\n",
              "  1005,\n",
              "  1049,\n",
              "  2469,\n",
              "  2045,\n",
              "  2024,\n",
              "  2216,\n",
              "  1997,\n",
              "  2017,\n",
              "  2041,\n",
              "  2045,\n",
              "  2040,\n",
              "  2228,\n",
              "  17690,\n",
              "  1019,\n",
              "  2003,\n",
              "  2204,\n",
              "  16596,\n",
              "  1011,\n",
              "  10882,\n",
              "  2694,\n",
              "  1012,\n",
              "  2009,\n",
              "  1005,\n",
              "  1055,\n",
              "  2025,\n",
              "  1012,\n",
              "  2009,\n",
              "  1005,\n",
              "  1055,\n",
              "  18856,\n",
              "  17322,\n",
              "  2094,\n",
              "  1998,\n",
              "  4895,\n",
              "  7076,\n",
              "  8197,\n",
              "  4892,\n",
              "  1012,\n",
              "  1007,\n",
              "  2096,\n",
              "  2149,\n",
              "  7193,\n",
              "  2453,\n",
              "  2066,\n",
              "  7603,\n",
              "  1998,\n",
              "  2839,\n",
              "  2458,\n",
              "  1010,\n",
              "  16596,\n",
              "  1011,\n",
              "  10882,\n",
              "  2003,\n",
              "  1037,\n",
              "  6907,\n",
              "  2008,\n",
              "  2515,\n",
              "  2025,\n",
              "  2202,\n",
              "  2993,\n",
              "  5667,\n",
              "  1006,\n",
              "  12935,\n",
              "  1012,\n",
              "  2732,\n",
              "  10313,\n",
              "  1007,\n",
              "  1012,\n",
              "  2009,\n",
              "  2089,\n",
              "  7438,\n",
              "  2590,\n",
              "  3314,\n",
              "  1010,\n",
              "  2664,\n",
              "  2025,\n",
              "  2004,\n",
              "  1037,\n",
              "  3809,\n",
              "  4695,\n",
              "  1012,\n",
              "  2009,\n",
              "  1005,\n",
              "  1055,\n",
              "  2428,\n",
              "  3697,\n",
              "  2000,\n",
              "  2729,\n",
              "  2055,\n",
              "  1996,\n",
              "  3494,\n",
              "  2182,\n",
              "  2004,\n",
              "  2027,\n",
              "  2024,\n",
              "  2025,\n",
              "  3432,\n",
              "  13219,\n",
              "  1010,\n",
              "  2074,\n",
              "  4394,\n",
              "  1037,\n",
              "  12125,\n",
              "  1997,\n",
              "  2166,\n",
              "  1012,\n",
              "  2037,\n",
              "  4506,\n",
              "  1998,\n",
              "  9597,\n",
              "  2024,\n",
              "  4799,\n",
              "  1998,\n",
              "  21425,\n",
              "  1010,\n",
              "  2411,\n",
              "  9145,\n",
              "  2000,\n",
              "  3422,\n",
              "  1012,\n",
              "  1996,\n",
              "  11153,\n",
              "  1997,\n",
              "  3011,\n",
              "  2113,\n",
              "  2009,\n",
              "  1005,\n",
              "  1055,\n",
              "  29132,\n",
              "  2004,\n",
              "  2027,\n",
              "  2031,\n",
              "  2000,\n",
              "  2467,\n",
              "  2360,\n",
              "  1000,\n",
              "  4962,\n",
              "  8473,\n",
              "  4181,\n",
              "  9766,\n",
              "  1005,\n",
              "  1055,\n",
              "  3011,\n",
              "  1012,\n",
              "  1012,\n",
              "  1012,\n",
              "  1000,\n",
              "  4728,\n",
              "  2111,\n",
              "  2052,\n",
              "  2025,\n",
              "  3613,\n",
              "  3666,\n",
              "  1012,\n",
              "  8473,\n",
              "  4181,\n",
              "  9766,\n",
              "  1005,\n",
              "  1055,\n",
              "  11289,\n",
              "  2442,\n",
              "  2022,\n",
              "  3810,\n",
              "  1999,\n",
              "  2037,\n",
              "  8753,\n",
              "  2004,\n",
              "  2023,\n",
              "  10634,\n",
              "  1010,\n",
              "  10036,\n",
              "  1010,\n",
              "  9996,\n",
              "  5493,\n",
              "  1006,\n",
              "  3666,\n",
              "  2009,\n",
              "  2302,\n",
              "  4748,\n",
              "  16874,\n",
              "  7807,\n",
              "  2428,\n",
              "  7545,\n",
              "  2023,\n",
              "  2188,\n",
              "  1007,\n",
              "  19817,\n",
              "  6784,\n",
              "  4726,\n",
              "  19817,\n",
              "  19736,\n",
              "  3372,\n",
              "  1997,\n",
              "  1037,\n",
              "  2265,\n",
              "  13891,\n",
              "  2015,\n",
              "  2046,\n",
              "  2686,\n",
              "  1012,\n",
              "  27594,\n",
              "  2121,\n",
              "  1012,\n",
              "  2061,\n",
              "  1010,\n",
              "  3102,\n",
              "  2125,\n",
              "  1037,\n",
              "  2364,\n",
              "  2839,\n",
              "  1012,\n",
              "  1998,\n",
              "  2059,\n",
              "  3288,\n",
              "  2032,\n",
              "  2067,\n",
              "  2004,\n",
              "  2178,\n",
              "  3364,\n",
              "  1012,\n",
              "  15333,\n",
              "  4402,\n",
              "  2480,\n",
              "  999,\n",
              "  5759,\n",
              "  2035,\n",
              "  2058,\n",
              "  2153,\n",
              "  1012,\n",
              "  102],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_imdb_10[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozXuQzlzNoq1"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xniYoauiNoq1"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZZnNUqeNoq2"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the ü§ó Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GB22F2o6Noq2"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abI1JqnoNoq2"
      },
      "source": [
        "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ICjpsPhINoq2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxGSNjrzNoq2"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjV0JCVRNoq2"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0iHhWS9Noq2"
      },
      "source": [
        "Before you start training your model, create a map of the expected ids to their labels with `id2label` and `label2id`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8YA1XVh1Noq2"
      },
      "outputs": [],
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6deZu6ETNoq2"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load DistilBERT with [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) along with the number of expected labels, and the label mappings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sYPVBpCPNoq2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc24f8766f334bacb06e07c96462f992",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 0.1256, 'train_samples_per_second': 398145.539, 'train_steps_per_second': 24892.059, 'train_loss': 0.0, 'epoch': 20.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=31260, training_loss=0.0, metrics={'train_runtime': 0.1256, 'train_samples_per_second': 398145.539, 'train_steps_per_second': 24892.059, 'train_loss': 0.0, 'epoch': 20.0})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"left_padding0model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    \n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    seed=42,\n",
        "    data_seed=123)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb_0[\"train\"],\n",
        "    eval_dataset=tokenized_imdb_0[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train(resume_from_checkpoint=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f580dc65a861409baa879d8c20419287",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60489ad0d2814c2ab9b1aef1e862d924",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NameError",
          "evalue": "name 'json' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\wyzhw\\Documents\\GitHub\\Bert_test\\bert_padding_test.ipynb ÂçïÂÖÉÊ†º 37\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y150sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     results[n] \u001b[39m=\u001b[39m result\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y150sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresults.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y150sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(results, f)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "results = {}\n",
        "\n",
        "for n in range(0, 11, 10):\n",
        "    result = trainer.evaluate()\n",
        "    results[n] = result\n",
        "\n",
        "\n",
        "with open(\"results.json\", \"w\") as f:\n",
        "    json.dump(results, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: {'eval_loss': 0.23403853178024292,\n",
              "  'eval_accuracy': 0.9166,\n",
              "  'eval_runtime': 357.8435,\n",
              "  'eval_samples_per_second': 69.863,\n",
              "  'eval_steps_per_second': 4.368,\n",
              "  'epoch': 2.0},\n",
              " 10: {'eval_loss': 0.23403853178024292,\n",
              "  'eval_accuracy': 0.9166,\n",
              "  'eval_runtime': 364.259,\n",
              "  'eval_samples_per_second': 68.632,\n",
              "  'eval_steps_per_second': 4.291,\n",
              "  'epoch': 2.0}}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30e5cbabac1447d59bf07571bf338019",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1914, 'learning_rate': 1.9360204734484968e-05, 'epoch': 0.06}\n",
            "{'loss': 0.1357, 'learning_rate': 1.872040946896993e-05, 'epoch': 0.13}\n",
            "{'loss': 0.1454, 'learning_rate': 1.8080614203454897e-05, 'epoch': 0.19}\n",
            "{'loss': 0.2537, 'learning_rate': 1.744081893793986e-05, 'epoch': 0.26}\n",
            "{'loss': 0.2634, 'learning_rate': 1.6801023672424827e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2562, 'learning_rate': 1.616122840690979e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2484, 'learning_rate': 1.5521433141394756e-05, 'epoch': 0.45}\n",
            "{'loss': 0.2546, 'learning_rate': 1.488163787587972e-05, 'epoch': 0.51}\n",
            "{'loss': 0.2496, 'learning_rate': 1.4241842610364684e-05, 'epoch': 0.58}\n",
            "{'loss': 0.2291, 'learning_rate': 1.3602047344849649e-05, 'epoch': 0.64}\n",
            "{'loss': 0.2437, 'learning_rate': 1.2962252079334613e-05, 'epoch': 0.7}\n",
            "{'loss': 0.2125, 'learning_rate': 1.2322456813819578e-05, 'epoch': 0.77}\n",
            "{'loss': 0.2148, 'learning_rate': 1.1682661548304543e-05, 'epoch': 0.83}\n",
            "{'loss': 0.2183, 'learning_rate': 1.1042866282789508e-05, 'epoch': 0.9}\n",
            "{'loss': 0.2242, 'learning_rate': 1.0403071017274472e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e55e1d0d37514cdfb9bf0a11c72ab435",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.22205103933811188, 'eval_runtime': 363.4682, 'eval_samples_per_second': 68.782, 'eval_steps_per_second': 4.3, 'epoch': 1.0}\n",
            "{'loss': 0.171, 'learning_rate': 9.763275751759437e-06, 'epoch': 1.02}\n",
            "{'loss': 0.1388, 'learning_rate': 9.123480486244403e-06, 'epoch': 1.09}\n",
            "{'loss': 0.1527, 'learning_rate': 8.483685220729368e-06, 'epoch': 1.15}\n",
            "{'loss': 0.1351, 'learning_rate': 7.843889955214333e-06, 'epoch': 1.22}\n",
            "{'loss': 0.144, 'learning_rate': 7.204094689699297e-06, 'epoch': 1.28}\n",
            "{'loss': 0.1467, 'learning_rate': 6.5642994241842614e-06, 'epoch': 1.34}\n",
            "{'loss': 0.1327, 'learning_rate': 5.924504158669226e-06, 'epoch': 1.41}\n",
            "{'loss': 0.1555, 'learning_rate': 5.284708893154191e-06, 'epoch': 1.47}\n",
            "{'loss': 0.1377, 'learning_rate': 4.644913627639156e-06, 'epoch': 1.54}\n",
            "{'loss': 0.128, 'learning_rate': 4.005118362124121e-06, 'epoch': 1.6}\n",
            "{'loss': 0.1279, 'learning_rate': 3.3653230966090854e-06, 'epoch': 1.66}\n",
            "{'loss': 0.1799, 'learning_rate': 2.72552783109405e-06, 'epoch': 1.73}\n",
            "{'loss': 0.1464, 'learning_rate': 2.085732565579015e-06, 'epoch': 1.79}\n",
            "{'loss': 0.1415, 'learning_rate': 1.4459373000639796e-06, 'epoch': 1.86}\n",
            "{'loss': 0.1378, 'learning_rate': 8.061420345489445e-07, 'epoch': 1.92}\n",
            "{'loss': 0.1131, 'learning_rate': 1.6634676903390917e-07, 'epoch': 1.98}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "571c5ebf748c4e28b5316831f7a29321",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.24687997996807098, 'eval_runtime': 362.9107, 'eval_samples_per_second': 68.887, 'eval_steps_per_second': 4.307, 'epoch': 2.0}\n",
            "{'train_runtime': 2814.2898, 'train_samples_per_second': 17.766, 'train_steps_per_second': 1.111, 'train_loss': 0.18125215174674378, 'epoch': 2.0}\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\wyzhw\\Documents\\GitHub\\Bert_test\\bert_padding_test.ipynb ÂçïÂÖÉÊ†º 36\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#X46sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# save the metrics\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#X46sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mlog_history[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#X46sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     result[\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m n\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#X46sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     result_df \u001b[39m=\u001b[39m result_df\u001b[39m.\u001b[39mappend(result, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#X46sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m csv_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmy_data.csv\u001b[39m\u001b[39m'\u001b[39m\n",
            "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "# define the range of n\n",
        "#for n in n_range:\n",
        "for n in range(0, 11, 10):\n",
        "    # use format method to create a variable name with n\n",
        "    variable = f\"tokenized_imdb_{n}\"\n",
        "    output_dir = f\"left_padding{n}model\"\n",
        "    logging_dir=f\"left_padding{n}model_logs\"\n",
        "\n",
        "    # use exec function to execute the assignment statement\n",
        "    var = eval(variable)\n",
        "    training_args = TrainingArguments(\n",
        "        \n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        \n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "\n",
        "        push_to_hub=True,\n",
        "        seed=42,\n",
        "        data_seed=123,\n",
        "        output_dir=output_dir,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=100\n",
        "    )\n",
        "    \n",
        "    # define the trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=var[\"train\"],\n",
        "        eval_dataset=var[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\wyzhw\\Documents\\GitHub\\Bert_test\\bert_padding_test.ipynb ÂçïÂÖÉÊ†º 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y135sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mevaluate()\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py:3011\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3008\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   3010\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3011\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   3012\u001b[0m     eval_dataloader,\n\u001b[0;32m   3013\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   3014\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3015\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3016\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   3017\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   3018\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   3019\u001b[0m )\n\u001b[0;32m   3021\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   3022\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\trainer.py:3190\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3188\u001b[0m observed_num_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   3189\u001b[0m \u001b[39m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 3190\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m   3191\u001b[0m     \u001b[39m# Update the observed num examples\u001b[39;00m\n\u001b[0;32m   3192\u001b[0m     observed_batch_size \u001b[39m=\u001b[39m find_batch_size(inputs)\n\u001b[0;32m   3193\u001b[0m     \u001b[39mif\u001b[39;00m observed_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\accelerate\\data_loader.py:460\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[39m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m         current_batch \u001b[39m=\u001b[39m send_to_device(current_batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    461\u001b[0m     next_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m batch_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_batches:\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\accelerate\\utils\\operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[1;32m--> 160\u001b[0m         {\n\u001b[0;32m    161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39mnon_blocking, skip_keys\u001b[39m=\u001b[39mskip_keys)\n\u001b[0;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    163\u001b[0m         }\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\accelerate\\utils\\operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[0;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[0;32m    160\u001b[0m         {\n\u001b[1;32m--> 161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[0;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    163\u001b[0m         }\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\accelerate\\utils\\operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[1;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mto(device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking)\n\u001b[0;32m    168\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mto(device)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cc7ae5fbed4492fa19fcf38bc351ff5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5a5ca7160e84bc1a3c6d320d2032a37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d052eb7812324a3daa0c7a1525682a25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba3b1663a346401daa7a01dff162573d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85f5df8b126e40da9226be77d73a46f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c90d785a6a3f409abeaae2ebc2e8ffe5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Realgon/left_padding100_model\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Realgon/left_padding100_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "for obj in trainer.state.log_history:\n",
        "    print(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\wyzhw\\Documents\\GitHub\\Bert_test\\bert_padding_test.ipynb ÂçïÂÖÉÊ†º 38\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y131sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mlog_history[\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
            "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "trainer.state.log_history['accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'train_runtime': 2814.2898,\n",
              "  'train_samples_per_second': 17.766,\n",
              "  'train_steps_per_second': 1.111,\n",
              "  'total_flos': 6564686875195392.0,\n",
              "  'train_loss': 0.18125215174674378,\n",
              "  'epoch': 2.0,\n",
              "  'step': 3126,\n",
              "  'n': 0}]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\wyzhw\\Documents\\GitHub\\Bert_test\\bert_padding_test.ipynb ÂçïÂÖÉÊ†º 38\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result[\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m n\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y125sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m result_df \u001b[39m=\u001b[39m result_df\u001b[39m.\u001b[39mappend(result, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "result['n'] = n\n",
        "result_df = result_df.append(result, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"left_padding50_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    seed=42,\n",
        "    data_seed=123\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb_50[\"train\"],\n",
        "    eval_dataset=tokenized_imdb_50[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34e5c891d2074e388487a232838686c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3348, 'learning_rate': 1.6801023672424827e-05, 'epoch': 0.32}\n",
            "{'loss': 0.2586, 'learning_rate': 1.3602047344849649e-05, 'epoch': 0.64}\n",
            "{'loss': 0.2306, 'learning_rate': 1.0403071017274472e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "972262025a1d48faaf28f4dd1f8cfaeb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.26562219858169556, 'eval_accuracy': 0.90268, 'eval_runtime': 377.3093, 'eval_samples_per_second': 66.259, 'eval_steps_per_second': 4.142, 'epoch': 1.0}\n",
            "{'loss': 0.1622, 'learning_rate': 7.204094689699297e-06, 'epoch': 1.28}\n",
            "{'loss': 0.1487, 'learning_rate': 4.005118362124121e-06, 'epoch': 1.6}\n",
            "{'loss': 0.1599, 'learning_rate': 8.061420345489445e-07, 'epoch': 1.92}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d75619eff2fc45cd874a4ea2be852e3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.235260471701622, 'eval_accuracy': 0.92952, 'eval_runtime': 376.2296, 'eval_samples_per_second': 66.449, 'eval_steps_per_second': 4.154, 'epoch': 2.0}\n",
            "{'train_runtime': 2855.1642, 'train_samples_per_second': 17.512, 'train_steps_per_second': 1.095, 'train_loss': 0.2124700436436512, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3126, training_loss=0.2124700436436512, metrics={'train_runtime': 2855.1642, 'train_samples_per_second': 17.512, 'train_steps_per_second': 1.095, 'train_loss': 0.2124700436436512, 'epoch': 2.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"left_padding50_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    seed=42,\n",
        "    data_seed=123\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb_50[\"train\"],\n",
        "    eval_dataset=tokenized_imdb_50[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/Realgon/left_padding50_model/tree/main/'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "679b7df4b7014bada4936185bc5d5a62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1975, 'learning_rate': 1.6801023672424827e-05, 'epoch': 0.32}\n",
            "{'loss': 0.1874, 'learning_rate': 1.3602047344849649e-05, 'epoch': 0.64}\n",
            "{'loss': 0.1809, 'learning_rate': 1.0403071017274472e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccf43e1ce4fd47669e32c87d95e3860f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2602356970310211, 'eval_accuracy': 0.9228, 'eval_runtime': 373.1317, 'eval_samples_per_second': 67.0, 'eval_steps_per_second': 4.189, 'epoch': 1.0}\n",
            "{'loss': 0.1007, 'learning_rate': 7.204094689699297e-06, 'epoch': 1.28}\n",
            "{'loss': 0.0853, 'learning_rate': 4.005118362124121e-06, 'epoch': 1.6}\n",
            "{'loss': 0.1099, 'learning_rate': 8.061420345489445e-07, 'epoch': 1.92}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d3fd9e8ba2f4e208618b8f37b876e92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.284171462059021, 'eval_accuracy': 0.93, 'eval_runtime': 372.6626, 'eval_samples_per_second': 67.085, 'eval_steps_per_second': 4.194, 'epoch': 2.0}\n",
            "{'train_runtime': 2858.2215, 'train_samples_per_second': 17.493, 'train_steps_per_second': 1.094, 'train_loss': 0.1417671253646099, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3126, training_loss=0.1417671253646099, metrics={'train_runtime': 2858.2215, 'train_samples_per_second': 17.493, 'train_steps_per_second': 1.094, 'train_loss': 0.1417671253646099, 'epoch': 2.0})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"left_padding100_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    seed=42,\n",
        "    data_seed=123\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb_100[\"train\"],\n",
        "    eval_dataset=tokenized_imdb_100[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/Realgon/left_padding100_model/tree/main/'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_It2XfONoq2"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\n",
        "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TyOjmT_bNoq2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ce9f123a82c431587f40fbd6f2725fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3126 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1597, 'learning_rate': 1.6801023672424827e-05, 'epoch': 0.32}\n",
            "{'loss': 0.1511, 'learning_rate': 1.3602047344849649e-05, 'epoch': 0.64}\n",
            "{'loss': 0.1564, 'learning_rate': 1.0403071017274472e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ab54e82836d4d89b2ec81b4e92d1c75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.2768949866294861, 'eval_accuracy': 0.92112, 'eval_runtime': 379.0743, 'eval_samples_per_second': 65.95, 'eval_steps_per_second': 4.123, 'epoch': 1.0}\n",
            "{'loss': 0.082, 'learning_rate': 7.204094689699297e-06, 'epoch': 1.28}\n",
            "{'loss': 0.0764, 'learning_rate': 4.005118362124121e-06, 'epoch': 1.6}\n",
            "{'loss': 0.0916, 'learning_rate': 8.061420345489445e-07, 'epoch': 1.92}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89ecd834a7924406a6e85f67f653c2f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3246845006942749, 'eval_accuracy': 0.92544, 'eval_runtime': 378.4581, 'eval_samples_per_second': 66.058, 'eval_steps_per_second': 4.13, 'epoch': 2.0}\n",
            "{'train_runtime': 2899.2484, 'train_samples_per_second': 17.246, 'train_steps_per_second': 1.078, 'train_loss': 0.11752973331981031, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3126, training_loss=0.11752973331981031, metrics={'train_runtime': 2899.2484, 'train_samples_per_second': 17.246, 'train_steps_per_second': 1.078, 'train_loss': 0.11752973331981031, 'epoch': 2.0})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"left_padding150_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    seed=42,\n",
        "    data_seed=123)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb_150[\"train\"],\n",
        "    eval_dataset=tokenized_imdb_150[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cef8a53f1a84305bc4dc5af5c9ea72e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4689 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0563, 'learning_rate': 5.07144380464918e-06, 'epoch': 2.24}\n",
            "{'loss': 0.0529, 'learning_rate': 2.9387929195990615e-06, 'epoch': 2.56}\n",
            "{'loss': 0.054, 'learning_rate': 8.061420345489445e-07, 'epoch': 2.88}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e9c0349d7da4c3a8f0d6ece4dfe933c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1563 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.38004031777381897, 'eval_accuracy': 0.92476, 'eval_runtime': 384.1011, 'eval_samples_per_second': 65.087, 'eval_steps_per_second': 4.069, 'epoch': 3.0}\n",
            "{'train_runtime': 1434.0528, 'train_samples_per_second': 52.299, 'train_steps_per_second': 3.27, 'train_loss': 0.01731584208889744, 'epoch': 3.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4689, training_loss=0.01731584208889744, metrics={'train_runtime': 1434.0528, 'train_samples_per_second': 52.299, 'train_steps_per_second': 3.27, 'train_loss': 0.01731584208889744, 'epoch': 3.0})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"left_padding150_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    seed=42,\n",
        "    data_seed=123)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb_150[\"train\"],\n",
        "    eval_dataset=tokenized_imdb_150[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Êó†Ê≥ïÂëàÁé∞‚Äúapplication/vnd.jupyter.widget-view+json‚ÄùÁöÑÂÜÖÂÆπ\n",
        "{\"model_id\":\"057a11bb382d417db9ab4c042ad62e75\",\"version_major\":2,\"version_minor\":0}\n",
        "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
        "{'loss': 0.3392, 'learning_rate': 1.6801023672424827e-05, 'epoch': 0.32}\n",
        "{'loss': 0.2638, 'learning_rate': 1.3602047344849649e-05, 'epoch': 0.64}\n",
        "{'loss': 0.2446, 'learning_rate': 1.0403071017274472e-05, 'epoch': 0.96}\n",
        "Êó†Ê≥ïÂëàÁé∞‚Äúapplication/vnd.jupyter.widget-view+json‚ÄùÁöÑÂÜÖÂÆπ\n",
        "{\"model_id\":\"ea2324d6b1874a199e5f632294202495\",\"version_major\":2,\"version_minor\":0}\n",
        "{'eval_loss': 0.3612774908542633, 'eval_accuracy': 0.86776, 'eval_runtime': 1436.2269, 'eval_samples_per_second': 17.407, 'eval_steps_per_second': 1.088, 'epoch': 1.0}\n",
        "{'loss': 0.1764, 'learning_rate': 7.204094689699297e-06, 'epoch': 1.28}\n",
        "{'loss': 0.1591, 'learning_rate': 4.005118362124121e-06, 'epoch': 1.6}\n",
        "{'loss': 0.1746, 'learning_rate': 8.061420345489445e-07, 'epoch': 1.92}\n",
        "Êó†Ê≥ïÂëàÁé∞‚Äúapplication/vnd.jupyter.widget-view+json‚ÄùÁöÑÂÜÖÂÆπ\n",
        "{\"model_id\":\"e11f558dc7c64eacb2d28408bf424940\",\"version_major\":2,\"version_minor\":0}\n",
        "{'eval_loss': 0.24108850955963135, 'eval_accuracy': 0.92488, 'eval_runtime': 746.9998, 'eval_samples_per_second': 33.467, 'eval_steps_per_second': 2.092, 'epoch': 2.0}\n",
        "{'train_runtime': 6886.1501, 'train_samples_per_second': 7.261, 'train_steps_per_second': 0.454, 'train_loss': 0.22320297629270353, 'epoch': 2.0}\n",
        "TrainOutput(global_step=3126, training_loss=0.22320297629270353, metrics={'train_runtime': 6886.1501, 'train_samples_per_second': 7.261, 'train_steps_per_second': 0.454, 'train_loss': 0.22320297629270353, 'epoch': 2.0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/Realgon/left_padding150_model/tree/main/'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbe734f03f6043e3a06dd165aaa2ca73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wyzhw\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "892a1e68f8774e9587732d4997d30b27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64e57c8f21a9459a812277473b31425f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5fb59a971194b7eb9659e4d6503267a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d46d4d9cffc346c2a2dbba81f6e457c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25a59f6f897c4b2e8a5e269746dc91d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "from evaluate import evaluator\n",
        "import evaluate\n",
        "\n",
        "pipe = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\", device=0)\n",
        "data = load_dataset(\"imdb\", split=\"test\").shuffle().select(range(1000))\n",
        "metric = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e809af40e31e47d9aec5b6ae8b917c85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "295ca602fc074770bc846268dc2b2f99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`data` is a preloaded Dataset! Ignoring `subset` and `split`.\n"
          ]
        },
        {
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'huggingface/Realgon/left_padding50_model'. Use `repo_type` argument if needed.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\wyzhw\\Documents\\GitHub\\Bert_test\\bert_padding_test.ipynb ÂçïÂÖÉÊ†º 55\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y142sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mevaluate\u001b[39;00m \u001b[39mimport\u001b[39;00m EvaluationSuite\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y142sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m suite \u001b[39m=\u001b[39m EvaluationSuite\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmathemakitten/sentiment-evaluation-suite\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y142sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m results \u001b[39m=\u001b[39m suite\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mhuggingface/Realgon/left_padding50_model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py:123\u001b[0m, in \u001b[0;36mEvaluationSuite.run\u001b[1;34m(self, model_or_pipeline)\u001b[0m\n\u001b[0;32m    121\u001b[0m args_for_task[\u001b[39m\"\u001b[39m\u001b[39msubset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39msubset\n\u001b[0;32m    122\u001b[0m args_for_task[\u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39msplit\n\u001b[1;32m--> 123\u001b[0m results \u001b[39m=\u001b[39m task_evaluator\u001b[39m.\u001b[39mcompute(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs_for_task)\n\u001b[0;32m    125\u001b[0m results[\u001b[39m\"\u001b[39m\u001b[39mtask_name\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m task_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m task\u001b[39m.\u001b[39msubset \u001b[39mif\u001b[39;00m task\u001b[39m.\u001b[39msubset \u001b[39melse\u001b[39;00m task_name\n\u001b[0;32m    126\u001b[0m results[\u001b[39m\"\u001b[39m\u001b[39mdata_preprocessor\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(task\u001b[39m.\u001b[39mdata_preprocessor) \u001b[39mif\u001b[39;00m task\u001b[39m.\u001b[39mdata_preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\evaluate\\evaluator\\text_classification.py:134\u001b[0m, in \u001b[0;36mTextClassificationEvaluator.compute\u001b[1;34m(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, second_input_column, label_column, label_mapping)\u001b[0m\n\u001b[0;32m    130\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_data(data\u001b[39m=\u001b[39mdata, subset\u001b[39m=\u001b[39msubset, split\u001b[39m=\u001b[39msplit)\n\u001b[0;32m    131\u001b[0m metric_inputs, pipe_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_data(\n\u001b[0;32m    132\u001b[0m     data\u001b[39m=\u001b[39mdata, input_column\u001b[39m=\u001b[39minput_column, second_input_column\u001b[39m=\u001b[39msecond_input_column, label_column\u001b[39m=\u001b[39mlabel_column\n\u001b[0;32m    133\u001b[0m )\n\u001b[1;32m--> 134\u001b[0m pipe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_pipeline(\n\u001b[0;32m    135\u001b[0m     model_or_pipeline\u001b[39m=\u001b[39;49mmodel_or_pipeline,\n\u001b[0;32m    136\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m    137\u001b[0m     feature_extractor\u001b[39m=\u001b[39;49mfeature_extractor,\n\u001b[0;32m    138\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    140\u001b[0m metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_metric(metric)\n\u001b[0;32m    142\u001b[0m \u001b[39m# Compute predictions\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\evaluate\\evaluator\\base.py:460\u001b[0m, in \u001b[0;36mEvaluator.prepare_pipeline\u001b[1;34m(self, model_or_pipeline, tokenizer, feature_extractor, device)\u001b[0m\n\u001b[0;32m    453\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_device()\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    456\u001b[0m     \u001b[39misinstance\u001b[39m(model_or_pipeline, \u001b[39mstr\u001b[39m)\n\u001b[0;32m    457\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(model_or_pipeline, transformers\u001b[39m.\u001b[39mPreTrainedModel)\n\u001b[0;32m    458\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(model_or_pipeline, transformers\u001b[39m.\u001b[39mTFPreTrainedModel)\n\u001b[0;32m    459\u001b[0m ):\n\u001b[1;32m--> 460\u001b[0m     pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m    461\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtask,\n\u001b[0;32m    462\u001b[0m         model\u001b[39m=\u001b[39;49mmodel_or_pipeline,\n\u001b[0;32m    463\u001b[0m         tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m    464\u001b[0m         feature_extractor\u001b[39m=\u001b[39;49mfeature_extractor,\n\u001b[0;32m    465\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m     \u001b[39mif\u001b[39;00m model_or_pipeline \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\pipelines\\__init__.py:747\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m     pretrained_model_name_or_path \u001b[39m=\u001b[39m model\n\u001b[0;32m    745\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig) \u001b[39mand\u001b[39;00m pretrained_model_name_or_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    746\u001b[0m     \u001b[39m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    748\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    749\u001b[0m         CONFIG_NAME,\n\u001b[0;32m    750\u001b[0m         _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    751\u001b[0m         _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    752\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    753\u001b[0m     )\n\u001b[0;32m    754\u001b[0m     hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    755\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\utils\\hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    428\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    431\u001b[0m         path_or_repo_id,\n\u001b[0;32m    432\u001b[0m         filename,\n\u001b[0;32m    433\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    434\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    435\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    436\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    437\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    438\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    439\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    440\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    441\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    442\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    443\u001b[0m     )\n\u001b[0;32m    444\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[0;32m    106\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[0;32m    108\u001b[0m ):\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 110\u001b[0m         validate_repo_id(arg_value)\n\u001b[0;32m    112\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\wyzhw\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRepo id must be a string, not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(repo_id)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[0;32m    164\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n",
            "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'huggingface/Realgon/left_padding50_model'. Use `repo_type` argument if needed."
          ]
        }
      ],
      "source": [
        "from evaluate import EvaluationSuite\n",
        "suite = EvaluationSuite.load('mathemakitten/sentiment-evaluation-suite')\n",
        "results = suite.run(\"huggingface/Realgon/left_padding50_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
              "Args:\n",
              "    predictions (`list` of `int`): Predicted labels.\n",
              "    references (`list` of `int`): Ground truth labels.\n",
              "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
              "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
              "\n",
              "Returns:\n",
              "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
              "\n",
              "Examples:\n",
              "\n",
              "    Example 1-A simple example\n",
              "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
              "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
              "        >>> print(results)\n",
              "        {'accuracy': 0.5}\n",
              "\n",
              "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
              "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
              "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
              "        >>> print(results)\n",
              "        {'accuracy': 3.0}\n",
              "\n",
              "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
              "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
              "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
              "        >>> print(results)\n",
              "        {'accuracy': 0.8778625954198473}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"Realgon/left_padding50_model\")\n",
        "data = load_dataset(\"imdb\", split=\"test\").shuffle().select(range(1000))\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDTOlZRYNoq2"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) applies dynamic padding by default when you pass `tokenizer` to it. In this case, you don't need to specify a data collator explicitly.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U3P2RFANoq3"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for text classification, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJEhPVGRNoq3"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzESQhAwNoq3"
      },
      "source": [
        "Great, now that you've finetuned a model, you can use it for inference!\n",
        "\n",
        "Grab some text you'd like to run inference on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "o28i4HbqNoq3"
      },
      "outputs": [],
      "source": [
        "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aORjdOjPNoq7"
      },
      "source": [
        "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for sentiment analysis with your model, and pass your text to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9963740706443787}]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"Realgon/left_padding50_model\")\n",
        "classifier(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'TextClassificationPipeline' object has no attribute 'state'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\wyzhw\\Documents\\GitHub\\Bert_test\\bert_padding_test.ipynb ÂçïÂÖÉÊ†º 61\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wyzhw/Documents/GitHub/Bert_test/bert_padding_test.ipynb#Y136sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39mlog_history\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'TextClassificationPipeline' object has no attribute 'state'"
          ]
        }
      ],
      "source": [
        "classifier.state.log_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmv4KpQWNoq7"
      },
      "source": [
        "You can also manually replicate the results of the `pipeline` if you'd like:\n",
        "\n",
        "Tokenize the text and return PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ewWH7KGiNoq7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Realgon/left_padding50_model\")\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tihdqbqtNoq8"
      },
      "source": [
        "Pass your inputs to the model and return the `logits`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wjHYsNCwNoq8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Realgon/left_padding50_model\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33nOlDGdNoq8"
      },
      "source": [
        "Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ENptObcoNoq8",
        "outputId": "29db9567-8c48-4842-ba9d-a511a7842ce5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'POSITIVE'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
